{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Preform evaluation on speech using computational evaluation tools such as PESQ, STOI and SDR both the reference (clean audio) and enhanced speech (mixture through models) are required. The enhanced speech is created with this program \n",
    "\n",
    "What is required:\n",
    "- Provide the CNN classification model from \"class_model.ipynb\"\n",
    "- Provide the GAN waveform generator from \"\"\n",
    "- Before performing step 4 is in the \"path_eval\" provide the clean and noisy audio files and the program will enhance the speech and evaluate through all iterations.\n",
    "- The name of the audio must be called speech_\"number\".wav the same as the create_dataset convention (With the text inside \"\" being changed to a number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cpu\n",
      "2.0.2+cpu\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import numpy as np\n",
    "import scipy.signal as sps\n",
    "import soundfile as sf\n",
    "import os\n",
    "import math\n",
    "import wave\n",
    "from scipy.io import wavfile\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "import xlsxwriter\n",
    "import openpyxl\n",
    "\n",
    "from pesq import pesq, NoUtterancesError\n",
    "from pystoi import stoi\n",
    "import mir_eval\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)\n",
    "\n",
    "%matplotlib inline\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "if physical_devices:\n",
    "  tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spectrogram(audio_file, image_file):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1)\n",
    "\n",
    "    y, sr = librosa.load(audio_file)\n",
    "    ms = librosa.feature.melspectrogram(y=y, sr=sr, S=None, n_fft=1024, hop_length=80, win_length=320, window='hann', center=True, pad_mode='constant', power=2.0)\n",
    "    #ms = librosa.feature.melspectrogram(y=y, sr=sr, S=None)\n",
    "    log_ms = librosa.power_to_db(ms, ref=np.max)\n",
    "    librosa.display.specshow(log_ms, sr=sr)\n",
    "\n",
    "    fig.savefig(image_file)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1) Loading CNN classification and GAN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 222, 222, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 111, 111, 32)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 109, 109, 128)     36992     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 54, 54, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 52, 52, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 26, 26, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 24, 24, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 12, 12, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 18432)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              18875392  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 8200      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 19,216,648\n",
      "Trainable params: 19,216,648\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Loading the CNN classification model\n",
    "# Recreate the exact same model, including its weights and the optimizer\n",
    "cnn_model = tf.keras.models.load_model('../Models/Classification-Models/my_model.h5')\n",
    "\n",
    "# Show the model architecture\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the noise GAN model\n",
    "Noise_gan = tf.saved_model.load('../Models/GAN-Models/Full_Curriculum_4_generator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAN noise subtraction\n",
    "def hann_window_a_signal(Windowed_data):\n",
    "    Hann_window = sps.windows.hann(len(Windowed_data))\n",
    "    Hann_Windowed_data = Hann_window * Windowed_data\n",
    "    padded_signal = np.pad(Hann_Windowed_data, (0, 512), 'constant')\n",
    "    Windowed_data_fft = np.fft.fft(padded_signal, 1024)\n",
    "    return Windowed_data_fft\n",
    "\n",
    "def perform_gan_noise_subtraction(input_audio_file, output_audio_file, alpha, Noise_gan):\n",
    "    N_fft = 1024\n",
    "\n",
    "    samplerate, data = wavfile.read(input_audio_file)\n",
    "    Bit_Check = wave.open(input_audio_file, 'rb')\n",
    "    bit_depth = Bit_Check.getsampwidth() * 8\n",
    "    data = data / (2 ** (bit_depth - 1))\n",
    "    Overlaps = math.floor(len(data) / 128)\n",
    "    audio_ss = np.zeros(len(data))\n",
    "\n",
    "    for No_of_overlaps in range(Overlaps - 5):\n",
    "        Rectangular_windowed_signal = data[0 + 128 * No_of_overlaps:512 + 128 * No_of_overlaps]\n",
    "        GAN_noise_estimate = np.zeros(N_fft)\n",
    "        FFT_of_windowed_signal = hann_window_a_signal(Rectangular_windowed_signal)\n",
    "\n",
    "        Hann_window = sps.windows.hann(len(Rectangular_windowed_signal))\n",
    "        PSD_window_scaling = np.sum(Hann_window ** 2)\n",
    "        PSD_of_windowed_signal = (np.abs(FFT_of_windowed_signal) ** 2) / (samplerate * PSD_window_scaling)\n",
    "\n",
    "        Tensor_PSD = tf.convert_to_tensor(PSD_of_windowed_signal.reshape(1, 1024), tf.float32)\n",
    "        Generated_codebook = Noise_gan(Tensor_PSD)\n",
    "        Generated_codebook = Generated_codebook.numpy()\n",
    "        Generated_codebook_reshaped = np.abs((Generated_codebook.reshape(1024, 9)))\n",
    "\n",
    "        Generated_codebook_inverse = np.linalg.pinv(Generated_codebook_reshaped, rcond=1e-15)\n",
    "        Generated_coeffs = Generated_codebook_inverse * PSD_of_windowed_signal\n",
    "        Generated_coeffs = np.transpose(Generated_coeffs)\n",
    "        GAN_noise_codebook = (Generated_coeffs * Generated_codebook_reshaped)\n",
    "        GAN_noise_codebook = GAN_noise_codebook.clip(min=0)\n",
    "\n",
    "        for Freq_bin in range(0, N_fft):\n",
    "            GAN_noise_estimate[Freq_bin] = np.sum(GAN_noise_codebook[Freq_bin, :])\n",
    "\n",
    "        GAN_noise_estimate[512:1024] = np.flip(GAN_noise_estimate[0:512])\n",
    "\n",
    "        snr = PSD_of_windowed_signal / (100 * GAN_noise_estimate)\n",
    "        Spectral_mask = 1 - np.minimum(1, np.maximum(0, alpha * snr))\n",
    "        Clean_signal = FFT_of_windowed_signal * Spectral_mask\n",
    "        Clean_frames = np.fft.ifft(Clean_signal)\n",
    "\n",
    "        audio_ss[0 + 128 * No_of_overlaps:512 + 128 * No_of_overlaps] = audio_ss[\n",
    "                                                                           0 + 128 * No_of_overlaps:512 + 128 * No_of_overlaps] + Clean_frames[0:512]\n",
    "\n",
    "    # Save the processed audio\n",
    "    sf.write(output_audio_file, audio_ss, samplerate, 'PCM_16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2) Function using compuatational evaluation tools such as PESQ, STOI and SDR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/audio/0.13.1/tutorials/mvdr_tutorial.html\n",
    "\n",
    "SDR means estimated speech contains more desired source signal and less undesired sources or noise (HIGHER FOR BETTER SPEECH QUALITY)\n",
    "\n",
    "The STOI score is a value between 0 and 1, where 0 represents no intelligibility (completely unintelligible) and 1 represents perfect intelligibility (no degradation, the speech is fully understandable).\n",
    "\n",
    "In the PESQ scale, the scores generally range from approximately -0.5 to 4.5, with specific meanings attributed to different score ranges: Scores above 4.0: Excellent quality. The degraded speech is very close to the reference, and the quality is perceived as excellent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 16000\n",
    "\n",
    "def si_snr(estimate, reference, epsilon=1e-8):\n",
    "    estimate = estimate - estimate.mean()\n",
    "    reference = reference - reference.mean()\n",
    "    reference_pow = reference.pow(2).mean(axis=1, keepdim=True)\n",
    "    mix_pow = (estimate * reference).mean(axis=1, keepdim=True)\n",
    "    scale = mix_pow / (reference_pow + epsilon)\n",
    "\n",
    "    reference = scale * reference\n",
    "    error = estimate - reference\n",
    "\n",
    "    reference_pow = reference.pow(2)\n",
    "    error_pow = error.pow(2)\n",
    "\n",
    "    reference_pow = reference_pow.mean(axis=1)\n",
    "    error_pow = error_pow.mean(axis=1)\n",
    "\n",
    "    si_snr = 10 * torch.log10(reference_pow) - 10 * torch.log10(error_pow)\n",
    "    return si_snr.item()\n",
    "\n",
    "def evaluate(estimate, reference):\n",
    "    try:\n",
    "        si_snr_score = si_snr(estimate, reference)\n",
    "        (\n",
    "            sdr,\n",
    "            _,\n",
    "            _,\n",
    "            _,\n",
    "        ) = mir_eval.separation.bss_eval_sources(reference.numpy(), estimate.numpy(), False)\n",
    "        pesq_mix = pesq(SAMPLE_RATE, estimate[0].numpy(), reference[0].numpy(), \"wb\")\n",
    "        stoi_mix = stoi(reference[0].numpy(), estimate[0].numpy(), SAMPLE_RATE, extended=False)\n",
    "        return pesq_mix, stoi_mix, sdr[0], si_snr_score\n",
    "    except NoUtterancesError as e:\n",
    "        return 0, 0, 0, 0  # Or any default values you prefer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3) Create directories to save the evaluation preformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Directory to save evaluation\n",
    "path_eval = \"Evaluation1\"\n",
    "\n",
    "isExist = os.path.exists(path_eval)\n",
    "if not isExist:\n",
    "    # Create a new directory because it does not exist\n",
    "    os.makedirs(path_eval)\n",
    "    os.makedirs(path_eval + \"/clean\")\n",
    "    os.makedirs(path_eval + \"/noisy\")\n",
    "    os.makedirs(path_eval + \"/enhanced\")\n",
    "    print(\"the %s directory is created!\", path_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The excel file 1 is created!\n"
     ]
    }
   ],
   "source": [
    "# Creating evaluation Excel file\n",
    "path_excel = path_eval + \"/evaluation.xlsx\"\n",
    "\n",
    "isExist3 = os.path.exists(path_excel)\n",
    "if not isExist3:\n",
    "   workbook1 = xlsxwriter.Workbook(path_excel)\n",
    "   worksheet1 = workbook1.add_worksheet()\n",
    "   worksheet1.write(0, 0, \"Number\")\n",
    "   worksheet1.write(0, 1, \"Name\")\n",
    "   worksheet1.write(0, 2, \"Original SNR(dB)\")\n",
    "   worksheet1.write(0, 3, \"Alpha Value\")\n",
    "   worksheet1.write(0, 4, \"SNR(dB)\")\n",
    "   worksheet1.write(0, 5, \"PESQ Evaluation\")\n",
    "   worksheet1.write(0, 6, \"STOI Evaluation\")\n",
    "   worksheet1.write(0, 7, \"SDR Evaluation\")\n",
    "   worksheet1.write(0, 8, \"SI-SDR Evaluation\")\n",
    "   worksheet1.write(0, 9, \"PESQ Highest\")\n",
    "   worksheet1.write(0, 10, \"STOI Highest\")\n",
    "   print(\"The excel file 1 is created!\")\n",
    "   workbook1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4) Using the clean and noisy audio to perform speech enhancement with evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Make sure to include the noisy and clean audio in the \"path_eval\" folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the clean audio folders\n",
    "clean_path = path_eval + \"/clean\"\n",
    "noisy_path = path_eval + \"/noisy\"\n",
    "enhanced_path = path_eval + \"/enhanced\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of noisy folders to enhance and evaluate: 2\n"
     ]
    }
   ],
   "source": [
    "# For all noisy files\n",
    "entries = os.listdir(noisy_path)\n",
    "print(\"Amount of noisy folders to enhance and evaluate: \" + str(len(entries)))\n",
    "\n",
    "noisy = [\"-3dB\", \"-6dB\", \"-9dB\", \"0dB\"] # Considered noisy classes\n",
    "clean = [\"3dB\", \"6dB\", \"9dB\", \"clean\"] # Considered clean classes\n",
    "target_names = [\"clean\", \"0dB\", \"-3dB\", \"-6dB\", \"-9dB\", \"3dB\", \"6dB\", \"9dB\"] # All classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speech_1: Alpha value: 0.00 Class: -3dB\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Edward\\AppData\\Local\\Temp\\ipykernel_21216\\472584710.py:49: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  audio_ss[0 + 128 * No_of_overlaps:512 + 128 * No_of_overlaps] = audio_ss[\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speech_1: Current SNR: Random, Alpha value: 2.00, Class: -3dB, PESQ: 1.130240797996521, STOI: 0.5481476040184929\n",
      "speech_2: Current SNR: Random, Alpha value: 0.00, Class: 9dB, PESQ: 1.0515708923339844, STOI: 0.6186998364983282\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for ent in entries:\n",
    "    edit_ent = ent.replace(\".wav\", \"\")\n",
    "    # Create a new directory at each noisy speech file\n",
    "    os.makedirs(enhanced_path + \"/{}/\".format(edit_ent) + \"spectrogram/\", exist_ok=True)\n",
    "    os.makedirs(enhanced_path + \"/{}/\".format(edit_ent) + \"wav/\", exist_ok=True)\n",
    "    new_ent_img = enhanced_path + \"/{}/\".format(edit_ent) + \"spectrogram/\"\n",
    "    new_ent_wav = enhanced_path + \"/{}/\".format(edit_ent) + \"wav/\"\n",
    "    create_spectrogram(noisy_path + \"/\" + ent, new_ent_img + 'noisy.png')\n",
    "    create_spectrogram(clean_path + \"/\" + ent, new_ent_img + 'clean.png')\n",
    "    current_image = new_ent_img + 'noisy.png' # Orignal noisy spectrogram image\n",
    "    noisy_wav = noisy_path + \"/\" + ent # Original noisy wav file\n",
    "    clean_wav = clean_path + \"/\" + ent # Original clean wav file\n",
    "    audio_clean = False\n",
    "    stop_value = 2 # Maximum alpha value\n",
    "    start_value = 0 # Minimum alpha value\n",
    "    step_value = 0.05 # Step size for alpha value\n",
    "    counter = 0 # Counter for number of iterations\n",
    "\n",
    "    STOI_Highest = 0\n",
    "    PESQ_Highest = 0\n",
    "    \n",
    "    while(audio_clean == False and start_value < stop_value):\n",
    "        counter += 1\n",
    "        # Preprocess the image\n",
    "        image = img_to_array(load_img(current_image, target_size=(224, 224, 3)))\n",
    "        image = image / 255.0\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "\n",
    "        # Make predictions\n",
    "        predictions = cnn_model.predict(image)\n",
    "        # Find the index of the maximum prediction\n",
    "        predicted_index = np.argmax(predictions)\n",
    "        # Check if the predicted class matches the target\n",
    "        predicted_class = target_names[predicted_index]\n",
    "\n",
    "        # Save the enhanced audio and spectrogram\n",
    "        start_value = round(start_value, 2)\n",
    "        print(\"{}: Alpha value: {} Class: {}\".format(edit_ent, \"{:.2f}\".format(start_value), predicted_class), end=\"\\r\")\n",
    "        output_audio = new_ent_wav + 'Alpha{}.wav'.format(\"{:.2f}\".format(start_value))\n",
    "        perform_gan_noise_subtraction(noisy_wav, output_audio, start_value, Noise_gan)\n",
    "        create_spectrogram(output_audio, new_ent_img + 'Alpha{}.png'.format(\"{:.2f}\".format(start_value)))\n",
    "        current_image = new_ent_img + 'Alpha{}.png'.format(\"{:.2f}\".format(start_value))\n",
    "        \n",
    "        # Evaluation\n",
    "        SAMPLE_CLEAN = clean_wav\n",
    "        SAMPLE_NOISY = output_audio\n",
    "        waveform_clean, sr = torchaudio.load(SAMPLE_CLEAN)\n",
    "        waveform_noisy, sr2 = torchaudio.load(SAMPLE_NOISY)\n",
    "\n",
    "        PESQ, STOI, SDR, SI_SNR_SCORE = evaluate(waveform_noisy[0:1], waveform_clean[0:1])\n",
    "\n",
    "        # Saving the best evaluation\n",
    "        if STOI > STOI_Highest:\n",
    "            STOI_Highest = STOI\n",
    "            STOI_saved = i \n",
    "        if PESQ > PESQ_Highest:\n",
    "            PESQ_Highest = PESQ\n",
    "            PESQ_saved = i\n",
    "\n",
    "        # Adding the evaluation information to the excel file\n",
    "        workfile1 = openpyxl.load_workbook(path_excel)\n",
    "        sheet1 = workfile1.active\n",
    "\n",
    "        sheet1.cell(row=i+2, column=1).value = counter\n",
    "        sheet1.cell(row=i+2, column=2).value = edit_ent\n",
    "        sheet1.cell(row=i+2, column=3).value = \"N/A\"\n",
    "        sheet1.cell(row=i+2, column=4).value = start_value\n",
    "        sheet1.cell(row=i+2, column=5).value = predicted_class\n",
    "        sheet1.cell(row=i+2, column=6).value = PESQ\n",
    "        sheet1.cell(row=i+2, column=7).value = STOI\n",
    "        sheet1.cell(row=i+2, column=8).value = SDR\n",
    "        sheet1.cell(row=i+2, column=9).value = SI_SNR_SCORE\n",
    "        workfile1.save(path_excel)\n",
    "        i += 1\n",
    "        \n",
    "        # Check if the audio is clean\n",
    "        if predicted_class in clean:\n",
    "            audio_clean = True\n",
    "        else: \n",
    "            start_value += step_value\n",
    "\n",
    "    workfile1 = openpyxl.load_workbook(path_excel)\n",
    "    sheet1 = workfile1.active\n",
    "\n",
    "    sheet1.cell(row=PESQ_saved+2, column=10).value = PESQ_Highest\n",
    "    sheet1.cell(row=STOI_saved+2, column=11).value = STOI_Highest\n",
    "\n",
    "    workfile1.save(path_excel)\n",
    "\n",
    "    i += 1\n",
    "\n",
    "    # Print the evaluation information\n",
    "    print(\"{}: Current SNR: {}, Alpha value: {}, Class: {}, PESQ: {}, STOI: {}\".format(edit_ent, \"N/A\", \"{:.2f}\".format(start_value), predicted_class, PESQ, STOI))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
